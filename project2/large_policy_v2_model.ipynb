{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Policy v2: Model-based approach\n",
    "\n",
    "Given unknown `(s, a, r, sp)` data, find optimal policy. Not all `(s, a)` pairs will be seen in data, so interpolate from neighbors.\n",
    "- States: |S| = 302020\n",
    "- Actions: 9 actions\n",
    "- Discount factor = 0.95\n",
    "\n",
    "Lisa Fung\n",
    "\n",
    "Last Updated: 11/9/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_data = pd.read_csv(\"./data/large.csv\")\n",
    "n_states = 302020\n",
    "n_actions = 9\n",
    "# n_limit_actions = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Data Observations\n",
    "\n",
    "Rewards\n",
    "- Only 7 unique values: [-10  -5   0   5  10  50 100]\n",
    "- r=100 only at states sp = 301013, 301111, via actions [1,4]\n",
    "- sp = 301013\n",
    "    - s=301012, a=1 (delta_s = +1)\n",
    "    - s=301014, a=2 (delta_s = -1)\n",
    "    - s=301113, a=3 (delta_s = -100)\n",
    "    - s=300413, a=4 (delta_s = +600)\n",
    "- sp = 301111\n",
    "    - s=301110, a=1 (delta_s = +1)\n",
    "    - s=301112, a=2 (delta_s = -1)\n",
    "    - s=301211, a=3 (delta_s = -100)\n",
    "    - s=301011, a=4 (delta_s = +100)\n",
    "\n",
    "\n",
    "Actions\n",
    "- a = [1,4] are probabilistic\n",
    "- a = [5,9] are usually 0, occasionally random\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "- Transition Model: $T(s' - s \\mid a)$\n",
    "- Rewards: $R(s, s')$\n",
    "- Only take actions $a = 1,2,3,4,5$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition Model\n",
    "\n",
    "$T(a, \\Delta s) = T(\\Delta s \\mid a)$\n",
    "- $|\\Delta s| = 9$, $|A| = 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_data['delta_s'] = large_data['sp'] - large_data['s']  # delta_s = sp - s\n",
    "\n",
    "# Only keep delta_s for actions [1, 4] and 0\n",
    "# array([-600, -100,   -6,   -1,    0,    1,    6,  100,  600])\n",
    "delta_s_list = np.sort(large_data[large_data['a'] == 1]['delta_s'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_delta_s = 9\n",
    "n_limit_actions = 5\n",
    "\n",
    "T = np.zeros((n_limit_actions+1, n_delta_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_data_a_delta_s = large_data[large_data['delta_s'].isin(delta_s_list)][['a', 'delta_s']].value_counts().sort_index(level=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, -600), 161)\n",
      "((1, -100), 562)\n",
      "((1, -6), 88)\n",
      "((1, -1), 637)\n",
      "((1, 0), 887)\n",
      "((1, 1), 6250)\n",
      "((1, 6), 1723)\n",
      "((1, 100), 603)\n",
      "((1, 600), 162)\n",
      "((2, -600), 177)\n",
      "((2, -100), 573)\n",
      "((2, -6), 1610)\n",
      "((2, -1), 6312)\n",
      "((2, 0), 885)\n",
      "((2, 1), 547)\n",
      "((2, 6), 175)\n",
      "((2, 100), 604)\n",
      "((2, 600), 153)\n",
      "((3, -600), 1574)\n",
      "((3, -100), 6440)\n",
      "((3, -6), 179)\n",
      "((3, -1), 581)\n",
      "((3, 0), 946)\n",
      "((3, 1), 596)\n",
      "((3, 6), 158)\n",
      "((3, 100), 558)\n",
      "((3, 600), 179)\n",
      "((4, -600), 99)\n",
      "((4, -100), 674)\n",
      "((4, -6), 198)\n",
      "((4, -1), 579)\n",
      "((4, 0), 764)\n",
      "((4, 1), 619)\n",
      "((4, 6), 155)\n",
      "((4, 100), 6345)\n",
      "((4, 600), 1648)\n",
      "((5, 0), 10913)\n",
      "((6, 0), 11131)\n",
      "((7, 0), 11033)\n",
      "((8, 0), 10957)\n",
      "((9, 0), 11149)\n"
     ]
    }
   ],
   "source": [
    "for row in large_data_a_delta_s.items():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Policy from Q Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract optimal policy pi(s) = a from action-value function Q(s, a)\n",
    "\n",
    "def extract_policy(Q, mode='random'):\n",
    "    \"\"\"\n",
    "    Limit only to actions 1-4.\n",
    "    \"\"\"\n",
    "    policy = np.zeros(n_states+1)\n",
    "    # predicable_action = np.random.randint(1, n_actions+1)\n",
    "    predicable_action = 4\n",
    "    for s in range(1, n_states+1):\n",
    "        policy[s] = np.argmax(Q[s, :])\n",
    "        if policy[s] not in [1, 2, 3, 4]: # Actions [5,9] are usually 0, random\n",
    "            if mode == 'random':\n",
    "                policy[s] = np.random.randint(1, 5)\n",
    "            if mode == 'previous':\n",
    "                policy[s] = predicable_action\n",
    "        else:\n",
    "            predicable_action = policy[s]\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = extract_policy(Q, mode='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimal_policy_sarsa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39munique(\u001b[43moptimal_policy_sarsa\u001b[49m, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimal_policy_sarsa' is not defined"
     ]
    }
   ],
   "source": [
    "np.unique(optimal_policy, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write optimal policy to file\n",
    "with open(\"large.policy\", \"w\") as file:\n",
    "    for a in optimal_policy[1:]:\n",
    "        file.write(f\"{int(a)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Improvements\n",
    "\n",
    "- Average values of Q(s, a) with some distance-dependent discount for unvisited states s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
