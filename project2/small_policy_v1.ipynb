{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Policy v1\n",
    "Given 10x10 grid world, (s, a, r, sp) data, find optimal policy\n",
    "\n",
    "Lisa Fung\n",
    "\n",
    "Last Updated: 11/5/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = pd.read_csv(\"./data/small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_data.head()\n",
    "# small_data.describe()\n",
    "# small_data.hist()\n",
    "# [print(col, small_data[col].value_counts()) for col in small_data.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small Data Information\n",
    "- States s: [1, 100], all seen\n",
    "- Actions a: [1, 4], all seen\n",
    "    - 1: left, 2: right, 3: up, 4: down\n",
    "- Rewards r: {0, 3, 10}\n",
    "- Next states sp: [1, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s   a  r \n",
       "82  2  3     154\n",
       "    4  3     137\n",
       "    3  3     135\n",
       "47  1  10    128\n",
       "    3  10    125\n",
       "    2  10    113\n",
       "    4  10    113\n",
       "82  1  3     105\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find R(s, a) > 0\n",
    "small_data_pos_rewards = small_data[small_data['r'] > 0]    # Subset of small data with positive R(s, a)\n",
    "\n",
    "# Find states, actions with positive rewards\n",
    "# R(82, a) = 3\n",
    "# R(47, a) = 10\n",
    "small_data_pos_rewards[['s', 'a', 'r']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal policy intuition: move toward 47, collect 82 along the way if possible\n",
    "- $R(82, a) = 3$\n",
    "- $R(47, a) = 10$\n",
    "- $R(s, a) = 0$  $\\forall s \\neq 82, 47$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Estimate transitions $T(s' | s, a)$ using **Maximum Likelihood Estimation** (Note: no exploration)\n",
    "2. Set rewards $R(s, a)$ with $R(47, a) = 10$, $R(82, a) = 3$, all other $R(s, a) = 0$\n",
    "3. Find optimal policy $\\pi^*$ using **Value Iteration**\n",
    "\n",
    "    a. Find $U^*(s)$ by updating $$U_{k+1}(s) = \\max_a ( R(s,a) + \\gamma * \\sum_{s'} T(s' | s,a) \\cdot U_k(s'))$$ until convergence when maximum change in value $||U_{k+1} - U_{k}||_{\\infty} < \\delta$\n",
    "\n",
    "    b. Extract $\\pi^*$ with $\\pi^*(s) = \\argmax_a ( R(s,a) + \\gamma * \\sum_{s'} T(s' | s,a) \\cdot U^*(s') )$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisaf\\AppData\\Local\\Temp\\ipykernel_25908\\3666276760.py:12: RuntimeWarning: invalid value encountered in divide\n",
      "  T /= np.sum(T, axis=2, keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "n_states = 100\n",
    "n_actions = 4\n",
    "\n",
    "# Estimate T(sp | s, a) in transition matrix T(s, a, sp)\n",
    "T = np.zeros((n_states + 1, n_actions + 1, n_states + 1))\n",
    "\n",
    "# Find counts N(s, a, sp)\n",
    "grouped_T = small_data.set_index(['s', 'a', 'sp']).groupby(by=['s', 'a', 'sp']).count()\n",
    "for indices, row in grouped_T.iterrows():\n",
    "    T[indices] = row.iloc[0]\n",
    "\n",
    "# Normalize along next state (sp) dimension to divide by N(s, a)\n",
    "T /= np.sum(T, axis=2, keepdims=True)\n",
    "T = np.nan_to_num(T, nan=0.0)   # convert nan to 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set rewards R(s, a) = R(s), only depends on current state\n",
    "R = np.zeros((n_states + 1))\n",
    "R[47] = 10\n",
    "R[82] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
